{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "test_env",
      "language": "python",
      "display_name": "Python (test_env)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "8 - od_detection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9105367a7e5b4f4e93595cf2077ce1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e03fa2f6a3774d78b5e312dc3df7328d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_117958ebec9644e4b70bad7fe43d7cb1",
              "IPY_MODEL_3613ccddf6c94b4d9a565330da8d30bd"
            ]
          }
        },
        "e03fa2f6a3774d78b5e312dc3df7328d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "117958ebec9644e4b70bad7fe43d7cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd3b009066d8435ab2bb3df1ee3853d6",
            "_dom_classes": [],
            "description": "Epoch 0: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4034247b45e0480fad826865d2af27ac"
          }
        },
        "3613ccddf6c94b4d9a565330da8d30bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22b82b73e936416283492a74f2d03a14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [00:17&lt;00:00,  1.72s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f6cef83a157a4649a4ea4aac5ef7f6a6"
          }
        },
        "cd3b009066d8435ab2bb3df1ee3853d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4034247b45e0480fad826865d2af27ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22b82b73e936416283492a74f2d03a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f6cef83a157a4649a4ea4aac5ef7f6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ec533e854764d2585f2ac859f3be335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b1dcae0d47045d2ad1dd63d6e930e62",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afeec9af00bc49858dd7b4a27fb60f82",
              "IPY_MODEL_ce4b34142bc44a30bcd0e5821044501e"
            ]
          }
        },
        "3b1dcae0d47045d2ad1dd63d6e930e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "afeec9af00bc49858dd7b4a27fb60f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba1375672c60462b815e692e03a751fe",
            "_dom_classes": [],
            "description": "Validating:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00dac66ba9dc4554b9f5ce8cfbfd556b"
          }
        },
        "ce4b34142bc44a30bcd0e5821044501e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d2b8cf9f928543adbc34a4d18c202bee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/4 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79c6b81d6d9446c4ae12e9f5757d68ab"
          }
        },
        "ba1375672c60462b815e692e03a751fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00dac66ba9dc4554b9f5ce8cfbfd556b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2b8cf9f928543adbc34a4d18c202bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79c6b81d6d9446c4ae12e9f5757d68ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Fh4o3dJ1XUQq"
      },
      "source": [
        "# Train your own object detector with Faster-RCNN & PyTorch: Heads detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu6-KFsUXkWp",
        "outputId": "b02c50ea-261f-454b-ff05-5a2d7e9c64e6"
      },
      "source": [
        "!unzip input.zip\n",
        "!unzip target.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  input.zip\n",
            "  inflating: input/000.jpg           \n",
            "  inflating: input/001.jpg           \n",
            "  inflating: input/002.jpg           \n",
            "  inflating: input/003.png           \n",
            "  inflating: input/004.jpg           \n",
            "  inflating: input/005.jpg           \n",
            "  inflating: input/006.jpg           \n",
            "  inflating: input/007.jpg           \n",
            "  inflating: input/008.jpg           \n",
            "  inflating: input/009.jpg           \n",
            "  inflating: input/010.jpg           \n",
            "  inflating: input/011.jpg           \n",
            "  inflating: input/012.jpg           \n",
            "  inflating: input/013.jpg           \n",
            "  inflating: input/014.jpg           \n",
            "  inflating: input/015.jpg           \n",
            "  inflating: input/016.jpg           \n",
            "  inflating: input/017.jpg           \n",
            "  inflating: input/018.jpg           \n",
            "  inflating: input/019.jpg           \n",
            "Archive:  target.zip\n",
            "  inflating: target/000.pt           \n",
            "  inflating: target/001.pt           \n",
            "  inflating: target/002.pt           \n",
            "  inflating: target/003.pt           \n",
            "  inflating: target/004.pt           \n",
            "  inflating: target/005.pt           \n",
            "  inflating: target/006.pt           \n",
            "  inflating: target/007.pt           \n",
            "  inflating: target/008.pt           \n",
            "  inflating: target/009.pt           \n",
            "  inflating: target/010.pt           \n",
            "  inflating: target/011.pt           \n",
            "  inflating: target/012.pt           \n",
            "  inflating: target/013.pt           \n",
            "  inflating: target/014.pt           \n",
            "  inflating: target/015.pt           \n",
            "  inflating: target/016.pt           \n",
            "  inflating: target/017.pt           \n",
            "  inflating: target/018.pt           \n",
            "  inflating: target/019.pt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7b5euxmZPRj",
        "outputId": "b1e0dab9-7d03-459d-f31c-e794c2910997"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/a1/a991780873b5fd760fb99dfda01916fe9e5b186f0ba70a120e6b4f79cfaa/pytorch_lightning-1.3.1-py3-none-any.whl (805kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 808kB 4.9MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e8/513cd9d0b1c83dc14cd8f788d05cd6a34758d4fd7e4f9e5ecd5d7d599c95/torchmetrics-0.3.2-py3-none-any.whl (274kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 37.2MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Collecting pyDeprecate==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n",
            "Collecting PyYAML<=5.4.1,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.8.1+cu101)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch_lightning) (2.23.0)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.30.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (56.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch_lightning) (2.10)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 42.6MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 44.3MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch_lightning) (21.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=f02324ed468b3b823ae617ae635f06ef8885c443fcbc7f0826a708e82eff2526\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: torchmetrics, multidict, yarl, async-timeout, aiohttp, fsspec, pyDeprecate, PyYAML, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.5.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.1 torchmetrics-0.3.2 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdcTSyuhXUQ1"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQWum5h4XUQ1"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from typing import List, Dict, Callable, Tuple\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision.ops import box_convert\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping #callbacks\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_YXJzYUYgea"
      },
      "source": [
        "def get_filenames_of_path(path: List[pathlib.Path], ext: str = '*'):\n",
        "    \"\"\"\n",
        "    Returns a list of files in a directory/path. Uses pathlib.\n",
        "    \"\"\"\n",
        "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
        "    return filenames\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vreNyjcmYBiC"
      },
      "source": [
        "\n",
        "class Compose:\n",
        "    \"\"\"Baseclass - composes several transforms together.\"\"\"\n",
        "\n",
        "    def __init__(self, transforms: List[Callable]):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __repr__(self): return str([transform for transform in self.transforms])\n",
        "    \n",
        "class ComposeDouble(Compose):\n",
        "    \"\"\"Composes transforms for input-target pairs.\"\"\"\n",
        "\n",
        "    def __call__(self, inp: np.ndarray, target: dict):\n",
        "        for t in self.transforms:\n",
        "            inp, target = t(inp, target)\n",
        "        return inp, target\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys7n7GBEYTZ1"
      },
      "source": [
        "class Repr:\n",
        "    \"\"\"Evaluatable string representation of an object\"\"\"\n",
        "\n",
        "    def __repr__(self): return f'{self.__class__.__name__}: {self.__dict__}'\n",
        "    \n",
        "class FunctionWrapperDouble(Repr):\n",
        "    \"\"\"A function wrapper that returns a partial for an input-target pair.\"\"\"\n",
        "\n",
        "    def __init__(self, function: Callable, input: bool = True, target: bool = False, *args, **kwargs):\n",
        "        from functools import partial\n",
        "        self.function = partial(function, *args, **kwargs)\n",
        "        self.input = input\n",
        "        self.target = target\n",
        "\n",
        "    def __call__(self, inp: np.ndarray, tar: dict):\n",
        "        if self.input: inp = self.function(inp)\n",
        "        if self.target: tar = self.function(tar)\n",
        "        return inp, tar\n",
        "        \n",
        "class AlbumentationWrapper(Repr):\n",
        "    \"\"\"\n",
        "    A wrapper for the albumentation package.\n",
        "    Bounding boxes are expected to be in xyxy format (pascal_voc).\n",
        "    Bounding boxes cannot be larger than the spatial image's dimensions.\n",
        "    Use Clip() if your bounding boxes are outside of the image, before using this wrapper.\n",
        "    \"\"\"\n",
        "    def __init__(self, albumentation: Callable, format: str = 'pascal_voc'):\n",
        "        self.albumentation = albumentation\n",
        "        self.format = format\n",
        "\n",
        "    def __call__(self, inp: np.ndarray, tar: dict):\n",
        "        # input, target\n",
        "        transform = A.Compose([\n",
        "            self.albumentation\n",
        "        ], bbox_params=A.BboxParams(format=self.format, label_fields=['class_labels']))\n",
        "\n",
        "        out_dict = transform(image=inp, bboxes=tar['boxes'], class_labels=tar['labels'])\n",
        "\n",
        "        input_out = np.array(out_dict['image'])\n",
        "        boxes = np.array(out_dict['bboxes'])\n",
        "        labels = np.array(out_dict['class_labels'])\n",
        "\n",
        "        tar['boxes'] = boxes\n",
        "        tar['labels'] = labels\n",
        "\n",
        "        return input_out, tar\n",
        "\n",
        "\n",
        "class Clip(Repr):\n",
        "    \"\"\"\n",
        "    If the bounding boxes exceed one dimension, they are clipped to the dim's maximum.\n",
        "    Bounding boxes are expected to be in xyxy format.\n",
        "    Example: x_value=224 but x_shape=200 -> x1=199\n",
        "    \"\"\"\n",
        "    def __call__(self, inp: np.ndarray, tar: dict):\n",
        "        new_boxes = clip_bbs(inp=inp, bbs=tar['boxes'])\n",
        "        tar['boxes'] = new_boxes\n",
        "\n",
        "        return inp, tar\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSE1tl3ayNd1"
      },
      "source": [
        "def map_class_to_int(labels: List[str], mapping: dict):\n",
        "    \"\"\"Maps a string to an integer.\"\"\"\n",
        "    labels = np.array(labels)\n",
        "    dummy = np.empty_like(labels)\n",
        "    for key, value in mapping.items():\n",
        "        dummy[labels == key] = value\n",
        "\n",
        "    return dummy.astype(np.uint8)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6EiB_w2yZ7s"
      },
      "source": [
        "def clip_bbs(inp: np.ndarray,\n",
        "             bbs: np.ndarray):\n",
        "    \"\"\"\n",
        "    If the bounding boxes exceed one dimension, they are clipped to the dim's maximum.\n",
        "    Bounding boxes are expected to be in xyxy format.\n",
        "    Example: x_value=224 but x_shape=200 -> x1=199\n",
        "    \"\"\"\n",
        "\n",
        "    def clip(value: int, max: int):\n",
        "\n",
        "        if value >= max - 1:\n",
        "            value = max - 1\n",
        "        elif value <= 0:\n",
        "            value = 0\n",
        "\n",
        "        return value\n",
        "\n",
        "    output = []\n",
        "    for bb in bbs:\n",
        "        x1, y1, x2, y2 = tuple(bb)\n",
        "        x_shape = inp.shape[1]\n",
        "        y_shape = inp.shape[0]\n",
        "\n",
        "        x1 = clip(x1, x_shape)\n",
        "        y1 = clip(y1, y_shape)\n",
        "        x2 = clip(x2, x_shape)\n",
        "        y2 = clip(y2, y_shape)\n",
        "\n",
        "        output.append([x1, y1, x2, y2])\n",
        "\n",
        "    return np.array(output)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq3WRXRQXqkL"
      },
      "source": [
        "class ObjectDetectionDataSet(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Builds a dataset with images and their respective targets.\n",
        "    A target is expected to be a pickled file of a dict\n",
        "    and should contain at least a 'boxes' and a 'labels' key.\n",
        "    inputs and targets are expected to be a list of pathlib.Path objects.\n",
        "\n",
        "    In case your labels are strings, you can use mapping (a dict) to int-encode them.\n",
        "    Returns a dict with the following keys: 'x', 'x_name', 'y', 'y_name'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 inputs: List[pathlib.Path],\n",
        "                 targets: List[pathlib.Path],\n",
        "                 transform: ComposeDouble = None,\n",
        "                 use_cache: bool = False,\n",
        "                 convert_to_format: str = None,\n",
        "                 mapping: Dict = None\n",
        "                 ):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "        self.use_cache = use_cache\n",
        "        self.convert_to_format = convert_to_format\n",
        "        self.mapping = mapping\n",
        "\n",
        "        if self.use_cache:\n",
        "            # Use multiprocessing to load images and targets into RAM\n",
        "            from multiprocessing import Pool\n",
        "            with Pool() as pool:\n",
        "                self.cached_data = pool.starmap(self.read_images, zip(inputs, targets))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self,\n",
        "                    index: int):\n",
        "        if self.use_cache:\n",
        "            x, y = self.cached_data[index]\n",
        "        else:\n",
        "            # Select the sample\n",
        "            input_ID = self.inputs[index]\n",
        "            target_ID = self.targets[index]\n",
        "\n",
        "            # Load input and target\n",
        "            x, y = self.read_images(input_ID, target_ID)\n",
        "\n",
        "        # From RGBA to RGB\n",
        "        if x.shape[-1] == 4:\n",
        "            from skimage.color import rgba2rgb\n",
        "            x = rgba2rgb(x)\n",
        "\n",
        "        # Read boxes\n",
        "        try:\n",
        "            boxes = torch.from_numpy(y['boxes']).to(torch.float32)\n",
        "        except TypeError:\n",
        "            boxes = torch.tensor(y['boxes']).to(torch.float32)\n",
        "\n",
        "        # Read scores\n",
        "        if 'scores' in y.keys():\n",
        "            try:\n",
        "                scores = torch.from_numpy(y['scores']).to(torch.float32)\n",
        "            except TypeError:\n",
        "                scores = torch.tensor(y['scores']).to(torch.float32)\n",
        "\n",
        "        # Label Mapping\n",
        "        if self.mapping:\n",
        "            labels = map_class_to_int(y['labels'], mapping=self.mapping)\n",
        "        else:\n",
        "            labels = y['labels']\n",
        "\n",
        "        # Read labels\n",
        "        try:\n",
        "            labels = torch.from_numpy(labels).to(torch.int64)\n",
        "        except TypeError:\n",
        "            labels = torch.tensor(labels).to(torch.int64)\n",
        "\n",
        "        # Convert format\n",
        "        if self.convert_to_format == 'xyxy':\n",
        "            boxes = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')  # transforms boxes from xywh to xyxy format\n",
        "        elif self.convert_to_format == 'xywh':\n",
        "            boxes = box_convert(boxes, in_fmt='xyxy', out_fmt='xywh')  # transforms boxes from xyxy to xywh format\n",
        "\n",
        "        # Create target\n",
        "        target = {'boxes': boxes,\n",
        "                  'labels': labels}\n",
        "\n",
        "        if 'scores' in y.keys():\n",
        "            target['scores'] = scores\n",
        "\n",
        "        # Preprocessing\n",
        "        target = {key: value.numpy() for key, value in target.items()}  # all tensors should be converted to np.ndarrays\n",
        "\n",
        "        if self.transform is not None:\n",
        "            x, target = self.transform(x, target)  # returns np.ndarrays\n",
        "\n",
        "        # Typecasting\n",
        "        x = torch.from_numpy(x).type(torch.float32)\n",
        "        target = {key: torch.from_numpy(value) for key, value in target.items()}\n",
        "\n",
        "        return {'x': x, 'y': target, 'x_name': self.inputs[index].name, 'y_name': self.targets[index].name}\n",
        "\n",
        "    @staticmethod\n",
        "    def read_images(inp, tar):\n",
        "        return imread(inp), torch.load(tar)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyuZGVfMYk5y"
      },
      "source": [
        "def normalize_01(inp: np.ndarray):\n",
        "    \"\"\"Squash image input to the value range [0, 1] (no clipping)\"\"\"\n",
        "    inp_out = (inp - np.min(inp)) / np.ptp(inp)\n",
        "    return inp_out"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K1OSFbrXUQ1"
      },
      "source": [
        "root = pathlib.Path('')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVJDcEDTXUQ2"
      },
      "source": [
        "inputs = get_filenames_of_path(root / 'input')\n",
        "targets = get_filenames_of_path(root / 'target')\n",
        "\n",
        "inputs.sort()\n",
        "targets.sort()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8JaO2v0XUQ2"
      },
      "source": [
        "mapping = {\n",
        "    'head': 1,\n",
        "}"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhktLCUwXUQ2"
      },
      "source": [
        "### Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AczG7qRzXUQ2"
      },
      "source": [
        "transforms = ComposeDouble([\n",
        "    Clip(),\n",
        "    # AlbumentationWrapper(albumentation=A.HorizontalFlip(p=0.5)),\n",
        "    # AlbumentationWrapper(albumentation=A.RandomScale(p=0.5, scale_limit=0.5)),\n",
        "    # AlbuWrapper(albu=A.VerticalFlip(p=0.5)),\n",
        "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
        "    FunctionWrapperDouble(normalize_01)\n",
        "])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HmhxrvcXUQ2"
      },
      "source": [
        "dataset = ObjectDetectionDataSet(inputs=inputs,\n",
        "                                 targets=targets,\n",
        "                                 transform=transforms,\n",
        "                                 use_cache=False,\n",
        "                                 convert_to_format=None,\n",
        "                                 mapping=mapping)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mtMqJuiXUQ8"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFql7VswY1M1"
      },
      "source": [
        "def collate_double(batch):\n",
        "    \"\"\"\n",
        "    collate function for the ObjectDetectionDataSet.\n",
        "    Only used by the dataloader.\n",
        "    \"\"\"\n",
        "    x = [sample['x'] for sample in batch]\n",
        "    y = [sample['y'] for sample in batch]\n",
        "    x_name = [sample['x_name'] for sample in batch]\n",
        "    y_name = [sample['y_name'] for sample in batch]\n",
        "    return x, y, x_name, y_name"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQWe-rwtXUQ9"
      },
      "source": [
        "# hyper-parameters\n",
        "params = {'BATCH_SIZE': 2,\n",
        "          'LR': 0.001,\n",
        "          'PRECISION': 32,\n",
        "          'CLASSES': 2,\n",
        "          'SEED': 42,\n",
        "          'PROJECT': 'Heads',\n",
        "          'EXPERIMENT': 'heads',\n",
        "          'MAXEPOCHS': 500,\n",
        "          'BACKBONE': 'resnet34',\n",
        "          'FPN': False,\n",
        "          'ANCHOR_SIZE': ((32, 64, 128, 256, 512),),\n",
        "          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n",
        "          'MIN_SIZE': 1024,\n",
        "          'MAX_SIZE': 1024,\n",
        "          'IMG_MEAN': [0.485, 0.456, 0.406],\n",
        "          'IMG_STD': [0.229, 0.224, 0.225],\n",
        "          'IOU_THRESHOLD': 0.5\n",
        "          }"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3UrSjAAXUQ-"
      },
      "source": [
        "# mapping\n",
        "mapping = {\n",
        "    'head': 1,\n",
        "}"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR5QJvqrXUQ-"
      },
      "source": [
        "# training transformations and augmentations\n",
        "transforms_training = ComposeDouble([\n",
        "    Clip(),\n",
        "    AlbumentationWrapper(albumentation=A.HorizontalFlip(p=0.5)),\n",
        "    AlbumentationWrapper(albumentation=A.RandomScale(p=0.5, scale_limit=0.5)),\n",
        "    # AlbuWrapper(albu=A.VerticalFlip(p=0.5)),\n",
        "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
        "    FunctionWrapperDouble(normalize_01)\n",
        "])\n",
        "\n",
        "# validation transformations\n",
        "transforms_validation = ComposeDouble([\n",
        "    Clip(),\n",
        "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
        "    FunctionWrapperDouble(normalize_01)\n",
        "])\n",
        "\n",
        "# test transformations\n",
        "transforms_test = ComposeDouble([\n",
        "    Clip(),\n",
        "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
        "    FunctionWrapperDouble(normalize_01)\n",
        "])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1hKSSrGXUQ_",
        "outputId": "e723c82d-518b-4798-e758-890a09421c4a"
      },
      "source": [
        "# random seed\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "seed_everything(params['SEED'])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1sgXZBLXUQ_"
      },
      "source": [
        "# training validation test split\n",
        "inputs_train, inputs_valid, inputs_test = inputs[:12], inputs[12:16], inputs[16:]\n",
        "targets_train, targets_valid, targets_test = targets[:12], targets[12:16], targets[16:]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9K2RoTGXUQ_"
      },
      "source": [
        "# dataset training\n",
        "dataset_train = ObjectDetectionDataSet(inputs=inputs_train,\n",
        "                                       targets=targets_train,\n",
        "                                       transform=transforms_training,\n",
        "                                       use_cache=True,\n",
        "                                       convert_to_format=None,\n",
        "                                       mapping=mapping)\n",
        "\n",
        "# dataset validation\n",
        "dataset_valid = ObjectDetectionDataSet(inputs=inputs_valid,\n",
        "                                       targets=targets_valid,\n",
        "                                       transform=transforms_validation,\n",
        "                                       use_cache=True,\n",
        "                                       convert_to_format=None,\n",
        "                                       mapping=mapping)\n",
        "\n",
        "# dataset test\n",
        "dataset_test = ObjectDetectionDataSet(inputs=inputs_test,\n",
        "                                      targets=targets_test,\n",
        "                                      transform=transforms_test,\n",
        "                                      use_cache=True,\n",
        "                                      convert_to_format=None,\n",
        "                                      mapping=mapping)\n",
        "\n",
        "# dataloader training\n",
        "dataloader_train = DataLoader(dataset=dataset_train,\n",
        "                              batch_size=params['BATCH_SIZE'],\n",
        "                              shuffle=True,\n",
        "                              num_workers=0,\n",
        "                              collate_fn=collate_double)\n",
        "\n",
        "# dataloader validation\n",
        "dataloader_valid = DataLoader(dataset=dataset_valid,\n",
        "                              batch_size=1,\n",
        "                              shuffle=False,\n",
        "                              num_workers=0,\n",
        "                              collate_fn=collate_double)\n",
        "\n",
        "# dataloader test\n",
        "dataloader_test = DataLoader(dataset=dataset_test,\n",
        "                             batch_size=1,\n",
        "                             shuffle=False,\n",
        "                             num_workers=0,\n",
        "                             collate_fn=collate_double)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXaWcB-4Z7kb"
      },
      "source": [
        "def get_resnet_backbone(backbone_name: str):\n",
        "    \"\"\"\n",
        "    Returns a resnet backbone pretrained on ImageNet.\n",
        "    Removes the average-pooling layer and the linear layer at the end.\n",
        "    \"\"\"\n",
        "    if backbone_name == 'resnet18':\n",
        "        pretrained_model = models.resnet18(pretrained=True, progress=False)\n",
        "        out_channels = 512\n",
        "    elif backbone_name == 'resnet34':\n",
        "        pretrained_model = models.resnet34(pretrained=True, progress=False)\n",
        "        out_channels = 512\n",
        "    elif backbone_name == 'resnet50':\n",
        "        pretrained_model = models.resnet50(pretrained=True, progress=False)\n",
        "        out_channels = 2048\n",
        "    elif backbone_name == 'resnet101':\n",
        "        pretrained_model = models.resnet101(pretrained=True, progress=False)\n",
        "        out_channels = 2048\n",
        "    elif backbone_name == 'resnet152':\n",
        "        pretrained_model = models.resnet152(pretrained=True, progress=False)\n",
        "        out_channels = 2048\n",
        "\n",
        "    backbone = torch.nn.Sequential(*list(pretrained_model.children())[:-2])\n",
        "    backbone.out_channels = out_channels\n",
        "\n",
        "    return backbone"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWv2xZlUaNZ_"
      },
      "source": [
        "def get_anchor_generator(anchor_size: Tuple[tuple] = None, aspect_ratios: Tuple[tuple] = None):\n",
        "    \"\"\"Returns the anchor generator.\"\"\"\n",
        "    if anchor_size is None:\n",
        "        anchor_size = ((16,), (32,), (64,), (128,))\n",
        "    if aspect_ratios is None:\n",
        "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_size)\n",
        "\n",
        "    anchor_generator = AnchorGenerator(sizes=anchor_size,\n",
        "                                       aspect_ratios=aspect_ratios)\n",
        "    return anchor_generator"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0mmup0FaeqR"
      },
      "source": [
        "def get_roi_pool(featmap_names: List[str] = None, output_size: int = 7, sampling_ratio: int = 2):\n",
        "    \"\"\"Returns the ROI Pooling\"\"\"\n",
        "    if featmap_names is None:\n",
        "        # default for resnet with FPN\n",
        "        featmap_names = ['0', '1', '2', '3']\n",
        "\n",
        "    roi_pooler = MultiScaleRoIAlign(featmap_names=featmap_names,\n",
        "                                    output_size=output_size,\n",
        "                                    sampling_ratio=sampling_ratio)\n",
        "\n",
        "    return roi_pooler"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrS_ii2pamup"
      },
      "source": [
        "def get_fasterRCNN(backbone: torch.nn.Module,\n",
        "                   anchor_generator: AnchorGenerator,\n",
        "                   roi_pooler: MultiScaleRoIAlign,\n",
        "                   num_classes: int,\n",
        "                   image_mean: List[float] = [0.485, 0.456, 0.406],\n",
        "                   image_std: List[float] = [0.229, 0.224, 0.225],\n",
        "                   min_size: int = 512,\n",
        "                   max_size: int = 1024,\n",
        "                   **kwargs\n",
        "                   ):\n",
        "    \"\"\"Returns the Faster-RCNN model. Default normalization: ImageNet\"\"\"\n",
        "    model = FasterRCNN(backbone=backbone,\n",
        "                       rpn_anchor_generator=anchor_generator,\n",
        "                       box_roi_pool=roi_pooler,\n",
        "                       num_classes=num_classes,\n",
        "                       image_mean=image_mean,  # ImageNet\n",
        "                       image_std=image_std,  # ImageNet\n",
        "                       min_size=min_size,\n",
        "                       max_size=max_size,\n",
        "                       **kwargs\n",
        "                       )\n",
        "    model.num_classes = num_classes\n",
        "    model.image_mean = image_mean\n",
        "    model.image_std = image_std\n",
        "    model.min_size = min_size\n",
        "    model.max_size = max_size\n",
        "\n",
        "    return model"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q-a_EgtZtcB"
      },
      "source": [
        "def get_fasterRCNN_resnet(num_classes: int,\n",
        "                          backbone_name: str,\n",
        "                          anchor_size: List[float],\n",
        "                          aspect_ratios: List[float],\n",
        "                          fpn: bool = True,\n",
        "                          min_size: int = 512,\n",
        "                          max_size: int = 1024,\n",
        "                          **kwargs\n",
        "                          ):\n",
        "    \"\"\"Returns the Faster-RCNN model with resnet backbone with and without fpn.\"\"\"\n",
        "\n",
        "    # Backbone\n",
        "    if fpn:\n",
        "        backbone = get_resnet_fpn_backbone(backbone_name=backbone_name)\n",
        "    else:\n",
        "        backbone = get_resnet_backbone(backbone_name=backbone_name)\n",
        "\n",
        "    # Anchors\n",
        "    anchor_size = anchor_size\n",
        "    aspect_ratios = aspect_ratios * len(anchor_size)\n",
        "    anchor_generator = get_anchor_generator(anchor_size=anchor_size, aspect_ratios=aspect_ratios)\n",
        "\n",
        "    # ROI Pool\n",
        "    with torch.no_grad():\n",
        "        backbone.eval()\n",
        "        random_input = torch.rand(size=(1, 3, 512, 512))\n",
        "        features = backbone(random_input)\n",
        "\n",
        "    if isinstance(features, torch.Tensor):\n",
        "        from collections import OrderedDict\n",
        "\n",
        "        features = OrderedDict([('0', features)])\n",
        "\n",
        "    featmap_names = [key for key in features.keys() if key.isnumeric()]\n",
        "\n",
        "    roi_pool = get_roi_pool(featmap_names=featmap_names)\n",
        "\n",
        "    # Model\n",
        "    return get_fasterRCNN(backbone=backbone,\n",
        "                          anchor_generator=anchor_generator,\n",
        "                          roi_pooler=roi_pool,\n",
        "                          num_classes=num_classes,\n",
        "                          min_size=min_size,\n",
        "                          max_size=max_size,\n",
        "                          **kwargs)\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6LXJEa0XURA"
      },
      "source": [
        "model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n",
        "                              backbone_name=params['BACKBONE'],\n",
        "                              anchor_size=params['ANCHOR_SIZE'],\n",
        "                              aspect_ratios=params['ASPECT_RATIOS'],\n",
        "                              fpn=params['FPN'],\n",
        "                              min_size=params['MIN_SIZE'],\n",
        "                              max_size=params['MAX_SIZE'])"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7m5nZwu4bzj"
      },
      "source": [
        "def from_dict_to_BoundingBox(file: dict, name: str, groundtruth: bool = True):\n",
        "    \"\"\"Returns list of BoundingBox objects from groundtruth or prediction.\"\"\"\n",
        "    from metrics.bounding_box import BoundingBox\n",
        "    from metrics.enumerators import BBFormat, BBType\n",
        "\n",
        "    labels = file['labels']\n",
        "    boxes = file['boxes']\n",
        "    scores = np.array(file['scores'].cpu()) if not groundtruth else [None] * len(boxes)\n",
        "\n",
        "    gt = BBType.GROUND_TRUTH if groundtruth else BBType.DETECTED\n",
        "\n",
        "    return [BoundingBox(image_name=name,\n",
        "                        class_id=int(l),\n",
        "                        coordinates=tuple(bb),\n",
        "                        format=BBFormat.XYX2Y2,\n",
        "                        bb_type=gt,\n",
        "                        confidence=s) for bb, l, s in zip(boxes, labels, scores)]\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm2b7-JFaxyM"
      },
      "source": [
        "class FasterRCNN_lightning(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 lr: float = 0.0001,\n",
        "                 iou_threshold: float = 0.5\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Model\n",
        "        self.model = model\n",
        "\n",
        "        # Classes (background inclusive)\n",
        "        self.num_classes = self.model.num_classes\n",
        "\n",
        "        # Learning rate\n",
        "        self.lr = lr\n",
        "\n",
        "        # IoU threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "\n",
        "        # Transformation parameters\n",
        "        self.mean = model.image_mean\n",
        "        self.std = model.image_std\n",
        "        self.min_size = model.min_size\n",
        "        self.max_size = model.max_size\n",
        "\n",
        "        # Save hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.model.eval()\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Batch\n",
        "        x, y, x_name, y_name = batch  # tuple unpacking\n",
        "\n",
        "        loss_dict = self.model(x, y)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        self.log_dict(loss_dict)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Batch\n",
        "        x, y, x_name, y_name = batch\n",
        "\n",
        "        # Inference\n",
        "        preds = self.model(x)\n",
        "\n",
        "        gt_boxes = [from_dict_to_BoundingBox(target, name=name, groundtruth=True) for target, name in zip(y, x_name)]\n",
        "        gt_boxes = list(chain(*gt_boxes))\n",
        "\n",
        "        pred_boxes = [from_dict_to_BoundingBox(pred, name=name, groundtruth=False) for pred, name in zip(preds, x_name)]\n",
        "        pred_boxes = list(chain(*pred_boxes))\n",
        "\n",
        "        return {'pred_boxes': pred_boxes, 'gt_boxes': gt_boxes}\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        gt_boxes = [out['gt_boxes'] for out in outs]\n",
        "        gt_boxes = list(chain(*gt_boxes))\n",
        "        pred_boxes = [out['pred_boxes'] for out in outs]\n",
        "        pred_boxes = list(chain(*pred_boxes))\n",
        "\n",
        "        from metrics.pascal_voc_evaluator import get_pascalvoc_metrics\n",
        "        from metrics.enumerators import MethodAveragePrecision\n",
        "        metric = get_pascalvoc_metrics(gt_boxes=gt_boxes,\n",
        "                                       det_boxes=pred_boxes,\n",
        "                                       iou_threshold=self.iou_threshold,\n",
        "                                       method=MethodAveragePrecision.EVERY_POINT_INTERPOLATION,\n",
        "                                       generate_table=True)\n",
        "\n",
        "        per_class, mAP = metric['per_class'], metric['mAP']\n",
        "        self.log('Validation_mAP', mAP)\n",
        "\n",
        "        for key, value in per_class.items():\n",
        "            self.log(f'Validation_AP_{key}', value['AP'])\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # Batch\n",
        "        x, y, x_name, y_name = batch\n",
        "\n",
        "        # Inference\n",
        "        preds = self.model(x)\n",
        "\n",
        "        gt_boxes = [from_dict_to_BoundingBox(target, name=name, groundtruth=True) for target, name in zip(y, x_name)]\n",
        "        gt_boxes = list(chain(*gt_boxes))\n",
        "\n",
        "        pred_boxes = [from_dict_to_BoundingBox(pred, name=name, groundtruth=False) for pred, name in zip(preds, x_name)]\n",
        "        pred_boxes = list(chain(*pred_boxes))\n",
        "\n",
        "        return {'pred_boxes': pred_boxes, 'gt_boxes': gt_boxes}\n",
        "\n",
        "    def test_epoch_end(self, outs):\n",
        "        gt_boxes = [out['gt_boxes'] for out in outs]\n",
        "        gt_boxes = list(chain(*gt_boxes))\n",
        "        pred_boxes = [out['pred_boxes'] for out in outs]\n",
        "        pred_boxes = list(chain(*pred_boxes))\n",
        "\n",
        "        from metrics.pascal_voc_evaluator import get_pascalvoc_metrics\n",
        "        from metrics.enumerators import MethodAveragePrecision\n",
        "        metric = get_pascalvoc_metrics(gt_boxes=gt_boxes,\n",
        "                                       det_boxes=pred_boxes,\n",
        "                                       iou_threshold=self.iou_threshold,\n",
        "                                       method=MethodAveragePrecision.EVERY_POINT_INTERPOLATION,\n",
        "                                       generate_table=True)\n",
        "\n",
        "        per_class, mAP = metric['per_class'], metric['mAP']\n",
        "        self.log('Test_mAP', mAP)\n",
        "\n",
        "        for key, value in per_class.items():\n",
        "            self.log(f'Test_AP_{key}', value['AP'])\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                    lr=self.lr,\n",
        "                                    momentum=0.9,\n",
        "                                    weight_decay=0.005)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                                  mode='max',\n",
        "                                                                  factor=0.75,\n",
        "                                                                  patience=30,\n",
        "                                                                  min_lr=0)\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler, 'monitor': 'Validation_mAP'}"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcz88cnsXURA"
      },
      "source": [
        "task = FasterRCNN_lightning(model=model, lr=params['LR'], iou_threshold=params['IOU_THRESHOLD'])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ1HCe0JXURA",
        "outputId": "1008f607-5651-471d-bd22-df65f5764d56"
      },
      "source": [
        "checkpoint_callback = ModelCheckpoint(monitor='Validation_mAP', mode='max')\n",
        "learningrate_callback = LearningRateMonitor(logging_interval='step', log_momentum=False)\n",
        "early_stopping_callback = EarlyStopping(monitor='Validation_mAP', patience=50, mode='max')\n",
        "\n",
        "# trainer init\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "trainer = Trainer(gpus=1,\n",
        "                  precision=params['PRECISION'],  # try 16 with enable_pl_optimizer=False\n",
        "                  callbacks=[checkpoint_callback, learningrate_callback, early_stopping_callback],\n",
        "                  default_root_dir='heads',  # where checkpoints are saved to\n",
        "                  log_every_n_steps=1,\n",
        "                  num_sanity_val_steps=0\n",
        "                  )"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710,
          "referenced_widgets": [
            "9105367a7e5b4f4e93595cf2077ce1a7",
            "e03fa2f6a3774d78b5e312dc3df7328d",
            "117958ebec9644e4b70bad7fe43d7cb1",
            "3613ccddf6c94b4d9a565330da8d30bd",
            "cd3b009066d8435ab2bb3df1ee3853d6",
            "4034247b45e0480fad826865d2af27ac",
            "22b82b73e936416283492a74f2d03a14",
            "f6cef83a157a4649a4ea4aac5ef7f6a6",
            "6ec533e854764d2585f2ac859f3be335",
            "3b1dcae0d47045d2ad1dd63d6e930e62",
            "afeec9af00bc49858dd7b4a27fb60f82",
            "ce4b34142bc44a30bcd0e5821044501e",
            "ba1375672c60462b815e692e03a751fe",
            "00dac66ba9dc4554b9f5ce8cfbfd556b",
            "d2b8cf9f928543adbc34a4d18c202bee",
            "79c6b81d6d9446c4ae12e9f5757d68ab"
          ]
        },
        "id": "5n1KJefsXURB",
        "outputId": "05c9949b-8122-4c7e-8cd8-daaf93b92d59"
      },
      "source": [
        "# start training\n",
        "trainer.max_epochs = params['MAXEPOCHS']\n",
        "trainer.fit(task,\n",
        "            train_dataloader=dataloader_train,\n",
        "            val_dataloaders=dataloader_valid)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type       | Params\n",
            "-------------------------------------\n",
            "0 | model | FasterRCNN | 50.4 M\n",
            "-------------------------------------\n",
            "50.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "50.4 M    Total params\n",
            "201.736   Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9105367a7e5b4f4e93595cf2077ce1a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec533e854764d2585f2ac859f3be335",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), mâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-610cccee8088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer.fit(task,\n\u001b[1;32m      4\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             val_dataloaders=dataloader_valid)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    456\u001b[0m         )\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_check_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, on_epoch)\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# capture any logged information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-68f51d56a60a>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfrom_dict_to_BoundingBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroundtruth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-68f51d56a60a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfrom_dict_to_BoundingBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroundtruth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-bafd2b912c64>\u001b[0m in \u001b[0;36mfrom_dict_to_BoundingBox\u001b[0;34m(file, name, groundtruth)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfrom_dict_to_BoundingBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroundtruth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns list of BoundingBox objects from groundtruth or prediction.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoundingBox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBBFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBBType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metrics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzzLMc_FXURB"
      },
      "source": [
        "# start testing\n",
        "trainer.test(ckpt_path='best', test_dataloaders=dataloader_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897LfuRr4d38"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}